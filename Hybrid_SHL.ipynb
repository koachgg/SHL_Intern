{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Environment Setup and Imports"
      ],
      "metadata": {
        "id": "NuzZPM7QTXT_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se0fWPTfTRsb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from IPython.display import FileLink, display\n",
        "\n",
        "# Force pure-Python protocol buffers to avoid TensorFlow proto conflicts.\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "import librosa\n",
        "import whisper\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import random\n",
        "import gc\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Regressors (choose one or experiment with several)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# For text embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "Ptc64eoITbrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Configuration and Utility Functions"
      ],
      "metadata": {
        "id": "5f8Zd_5uTlo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths – adjust these for your Kaggle environment\n",
        "CONFIG = {\n",
        "    \"train_csv\": \"/kaggle/input/shl-dataset/dataset/train.csv\",\n",
        "    \"test_csv\": \"/kaggle/input/shl-dataset/dataset/test.csv\",\n",
        "    \"audios_train\": \"/kaggle/input/shl-dataset/dataset/audios_train\",\n",
        "    \"audios_test\": \"/kaggle/input/shl-dataset/dataset/audios_test\",\n",
        "    \"sample_submission\": \"/kaggle/input/shl-dataset/dataset/sample_submission.csv\",\n",
        "    \"output_submission\": \"/kaggle/working/submission.csv\",\n",
        "    # Audio processing\n",
        "    \"target_sample_rate\": 16000,\n",
        "    \"max_audio_length\": 10,  # seconds\n",
        "}\n",
        "CONFIG[\"max_audio_length_samples\"] = CONFIG[\"target_sample_rate\"] * CONFIG[\"max_audio_length\"]\n",
        "\n",
        "# Utility download function (for Kaggle output)\n",
        "def download_file(path, download_file_name):\n",
        "    os.chdir('/kaggle/working/')\n",
        "    zip_name = f\"{download_file_name}.zip\"\n",
        "    command = f\"zip {zip_name} {path} -r\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"Unable to run zip command!\")\n",
        "        print(result.stderr)\n",
        "        return\n",
        "    display(FileLink(zip_name))"
      ],
      "metadata": {
        "id": "XCjCs2p3TkBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Hybrid Feature Extraction Functions"
      ],
      "metadata": {
        "id": "MNw_RvqhTuMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Compute acoustic features from audio using Whisper’s encoder\n",
        "def extract_acoustic_features(audio_path, whisper_model):\n",
        "    \"\"\"\n",
        "    Loads and processes audio for Whisper.\n",
        "    Uses Whisper's log-Mel spectrogram and encoder.\n",
        "    Returns a deep acoustic feature vector by mean-pooling.\n",
        "    \"\"\"\n",
        "    # Load and pad audio via Whisper utilities\n",
        "    audio = whisper.load_audio(audio_path)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Get encoder output; shape: [batch, frames, hidden_size]\n",
        "        encoded = whisper_model.encoder(mel.unsqueeze(0))\n",
        "    # Mean pool over time dimension to get a single feature vector\n",
        "    acoustic_feature = encoded.squeeze(0).mean(dim=0).cpu().numpy()\n",
        "    return acoustic_feature"
      ],
      "metadata": {
        "id": "s9O4P4u0TwVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Extract hand-crafted acoustic features using librosa\n",
        "def extract_handcrafted_features(audio_path, sr=16000):\n",
        "    \"\"\"\n",
        "    Loads audio with librosa and computes:\n",
        "      - MFCC means and standard deviations (n_mfcc=13)\n",
        "      - Zero crossing rate (mean and std)\n",
        "      - RMS energy (mean and std)\n",
        "    Returns a vector of handcrafted acoustic features.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, _ = librosa.load(audio_path, sr=sr)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        mfcc_mean = np.mean(mfcc, axis=1)\n",
        "        mfcc_std = np.std(mfcc, axis=1)\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "        rms = librosa.feature.rms(y=y)[0]\n",
        "        handcrafted = np.concatenate([mfcc_mean, mfcc_std, [np.mean(zcr), np.std(zcr)], [np.mean(rms), np.std(rms)]])\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting handcrafted features from {audio_path}: {e}\")\n",
        "        # If error, return zeros (length: 13+13+2+2 = 30)\n",
        "        handcrafted = np.zeros(30)\n",
        "    return handcrafted\n"
      ],
      "metadata": {
        "id": "ztcm-pkQTzdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Extract linguistic (text) features:\n",
        "def extract_text_features(audio_path, whisper_model, text_encoder):\n",
        "    \"\"\"\n",
        "    Uses Whisper to transcribe audio and a SentenceTransformer to encode the transcript.\n",
        "    Returns a text embedding vector.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = whisper_model.transcribe(audio_path, fp16=False)\n",
        "        transcript = result['text']\n",
        "        text_embed = text_encoder.encode(transcript)\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing or encoding text from {audio_path}: {e}\")\n",
        "        # If error, return zeros (assume text embeddings are length 768)\n",
        "        text_embed = np.zeros(768)\n",
        "    return text_embed"
      ],
      "metadata": {
        "id": "K741yKhFT2ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 Compute duration (in seconds) using librosa (or torchaudio)\n",
        "def compute_duration(audio_path, sr=16000):\n",
        "    try:\n",
        "        y, _ = librosa.load(audio_path, sr=sr)\n",
        "        duration = len(y) / sr\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing duration for {audio_path}: {e}\")\n",
        "        duration = 0.0\n",
        "    return duration\n"
      ],
      "metadata": {
        "id": "rOuQKmwAT4qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.5 Hybrid feature extraction:\n",
        "def extract_hybrid_features(df, audio_folder, whisper_model, text_encoder):\n",
        "    \"\"\"\n",
        "    For each audio file:\n",
        "      - Extract deep acoustic features (from Whisper encoder)\n",
        "      - Extract handcrafted acoustic features (MFCCs, ZCR, RMS)\n",
        "      - Extract text (linguistic) features (Whisper transcription + SentenceTransformer)\n",
        "      - Compute duration\n",
        "    Then concatenates all feature vectors into one combined feature vector.\n",
        "    \"\"\"\n",
        "    combined_features = []\n",
        "    for file in tqdm(df['filename'], desc=\"Extracting hybrid features\"):\n",
        "        file_path = os.path.join(audio_folder, file)\n",
        "        # Deep acoustic representation (e.g., 512-dim or similar)\n",
        "        acoustic_feat = extract_acoustic_features(file_path, whisper_model)\n",
        "        # Handcrafted features (30-dimensional, as defined above)\n",
        "        handcrafted_feat = extract_handcrafted_features(file_path, sr=CONFIG[\"target_sample_rate\"])\n",
        "        # Text features (e.g., 768-dim from SentenceTransformer)\n",
        "        text_feat = extract_text_features(file_path, whisper_model, text_encoder)\n",
        "        # Duration as scalar\n",
        "        duration = compute_duration(file_path, sr=CONFIG[\"target_sample_rate\"])\n",
        "        # Optionally, you can normalize duration (e.g., divide by 60) later during training\n",
        "        # Concatenate all features into one vector\n",
        "        features = np.concatenate([acoustic_feat, handcrafted_feat, text_feat, [duration]])\n",
        "        combined_features.append(features)\n",
        "    combined_features = np.array(combined_features)\n",
        "    return combined_features\n"
      ],
      "metadata": {
        "id": "BnCc-cj5T81t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Data Loading and Preparation"
      ],
      "metadata": {
        "id": "1XX1LhmtT-4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV files\n",
        "train_df = pd.read_csv(CONFIG[\"train_csv\"])\n",
        "test_df = pd.read_csv(CONFIG[\"test_csv\"])\n",
        "\n",
        "# Create full paths for audio files\n",
        "train_df['file_path'] = train_df['filename'].apply(lambda x: os.path.join(CONFIG[\"audios_train\"], x))\n",
        "test_df['file_path']  = test_df['filename'].apply(lambda x: os.path.join(CONFIG[\"audios_test\"], x))\n",
        "\n",
        "# Load models for feature extraction\n",
        "print(\"Loading Whisper model (for both acoustic and transcription) ...\")\n",
        "whisper_model = whisper.load_model(\"base\").to(device)\n",
        "print(\"Loading SentenceTransformer model for text embeddings ...\")\n",
        "text_encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "\n",
        "# Extract hybrid features for training\n",
        "print(\"Extracting hybrid features for training ...\")\n",
        "X = extract_hybrid_features(train_df, CONFIG[\"audios_train\"], whisper_model, text_encoder)\n",
        "y = train_df['label'].values"
      ],
      "metadata": {
        "id": "s-Hgm5ASUD6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Train-Validation Split and Regressor Training"
      ],
      "metadata": {
        "id": "g4_1aqOhUKzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Splitting data into training and validation sets ...\")\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Choose a regressor; for example, XGBoost\n",
        "print(\"Training XGBoost Regressor on hybrid features ...\")\n",
        "model_xgb = XGBRegressor(n_estimators=400, learning_rate=0.009, max_depth=6, random_state=42)\n",
        "model_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "val_preds = model_xgb.predict(X_val)\n",
        "rmse = mean_squared_error(y_val, val_preds, squared=False)\n",
        "print(f\"Validation RMSE (XGBoost): {rmse:.4f}\")\n",
        "\n",
        "# Alternatively, you can try an MLP:\n",
        "print(\"Training MLP Regressor on hybrid features ...\")\n",
        "model_mlp = MLPRegressor(hidden_layer_sizes=(512,464,256), activation='tanh',\n",
        "                         solver='sgd', max_iter=500000, random_state=42)\n",
        "model_mlp.fit(X_train, y_train)\n",
        "val_preds_mlp = model_mlp.predict(X_val)\n",
        "rmse_mlp = mean_squared_error(y_val, val_preds_mlp, squared=False)\n",
        "print(f\"Validation RMSE (MLP Neural Net): {rmse_mlp:.4f}\")"
      ],
      "metadata": {
        "id": "sTxununqULJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Inference on Test Set and Submission Creation"
      ],
      "metadata": {
        "id": "fFtn5iffUP9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting hybrid features for test set ...\")\n",
        "X_test = extract_hybrid_features(test_df, CONFIG[\"audios_test\"], whisper_model, text_encoder)\n",
        "print(\"Predicting on test set with XGBoost ...\")\n",
        "test_preds = model_xgb.predict(X_test)\n",
        "test_preds = np.clip(test_preds, 0, 5)  # Clip to valid range if necessary\n",
        "# (Optional) Smoothing: combine with overall training mean, e.g.:\n",
        "test_preds = 0.9 * test_preds + 0.1 * y_train.mean()\n",
        "\n",
        "# Create and save submission file\n",
        "submission_df = pd.DataFrame({\n",
        "    \"filename\": test_df[\"filename\"],\n",
        "    \"label\": test_preds\n",
        "})\n",
        "submission_df.to_csv(CONFIG[\"output_submission\"], index=False)\n",
        "print(\"Submission file saved!\")\n",
        "print(\"Submission file path:\", os.path.abspath(CONFIG[\"output_submission\"]))\n",
        "download_file(CONFIG[\"output_submission\"], \"out\")"
      ],
      "metadata": {
        "id": "_Fc6n8LQUSZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}